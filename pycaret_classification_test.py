# -*- coding: utf-8 -*-
"""PyCaret_Classification_test

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LNmS6QsjnWQN-X96PyahLsHvf_RtKA7u

# Machine learning with PyCaret
### Popular ML Libraries

+ Scikit-learn
+ Tensorflow
+ Keras
+ Pytorch
+ Caffe
+ PyCaret

## PyCaret

+ PyCaret is an open source, low-code machine learning library in Python that allows you to go from preparing your data to deploying your model within seconds in your choice of notebook environment.
+ It is a wrapper around several popular ML and Data Science libraries
+ Official website: [https://pycaret.org](https://pycaret.org/)

### Installation

+ !pip install pycaret

### Workflow

+ Prep Data
+ Initialize Setup
 + Define the data and the target class
+ Compare Model
+ Create Model
 + Select the one you want
+ Check accuracy of a selected model -predict
+ Tune model
+ Evaluate model
+ Intepret Model
+ Save model
"""

# Installation
!pip install pycaret

# Load EDA Pkgs
import pandas as pd

# Commented out IPython magic to ensure Python compatibility.
# Matplolib Environments
# %matplotlib --list

# Commented out IPython magic to ensure Python compatibility.
# Matplolib inline
# %matplotlib inline

# Load Dataset Method 1
from google.colab import files 
files.upload()

"""We use the 'heart_failure_clinical_records_dataset.csv' from [Heart failure clinical records Data Set](https://archive.ics.uci.edu/ml/datasets/Heart+failure+clinical+records)"""

df = pd.read_csv("heart_failure_clinical_records_dataset.csv")

df.head()

# Load Dataset Method2
# Datasource
data_url = "https://raw.githubusercontent.com/Jcharis/data-science-projects/master/data-science-projects/notebooks/data/heart_failure_clinical_records_dataset.csv"

# Load Dataset
df = pd.read_csv(data_url)

# Shape
df.shape

# Check for missing values
df.isnull().sum()

# Columns
df.columns

# Rename A Column
df.rename(columns={'DEATH_EVENT':'class'},inplace=True)

df.columns

# Descriptive Stas
df.describe()

# Value Count Plot
df['class'].value_counts()

df['class'].value_counts().plot(kind='bar')

"""## Using PyCaret for ML"""

import pycaret.classification

# Methods/Attrib
dir(pycaret.classification)

# Simplify way
import pycaret.classification as pc

dir(pc)

"""### Initialize or Setup

+ setup()
+ initializes the environment in pycaret
+ creates the transformation pipeline to prepare the data for ML
+ from pycaret.utils import enable_colab
+ enable_colab()
"""

from pycaret.utils import enable_colab
enable_colab()

# Init
# Enter Y for all variables in dataset
# Specify % of data for train/test/split
clf = pc.setup(data=df,target='class')

# Ignore A Column
pc.setup(data=df,target='class',ignore_features=['age','diabetes'])

# We use all features
pc.setup(data=df,target='class')

"""### Compare Multiple Models and their Accuracy Metrics

+ Similar to Classification Report,AUC,F1 Score

+ For Classification Problems
    + Classification report
    + AUC,Recall,Precision,F1 score, Kappa

+ For Regression Problems
    + MAE, MSE, RMSE, R2, RMSLE and MAPE
"""

# Compare models
pc.compare_models()

# Compare models
# But ignore the models in the blacklist
pc.compare_models(blacklist=['svm'])

"""#### Narative
+ Pycaret builds a model using several algorithms and compare the best
+ It automatically sort them from the best accuracy to the least
+ It highlight the best model according to the classification report metrics

### Creating the Model

+ Select the best model
+ Cross Validation
+ Perform CV K-Fold (10 default) for the selected model
"""

!pip install neatutils

# Simple Tools to Get The Short/Abbrev for an Estimator/Ml Algorithm
#! pip install neatutils
import neatutils
neatutils.get_abbrev('Logistic Regression')

neatutils.get_abbrev('Extreme Gradient Boosting')

# Create the model
xgboost_model = pc.create_model('xgboost')

# LogReg Model
logreg_model = pc.create_model('lr')

# Tune the Model
tuned_xgboost = pc.tune_model('xgboost')

# Let's compare the model (original and tuned)
print(xgboost_model)

print(tuned_xgboost)

# Optimize The Model 
tuned_xgb_optimized = pc.tune_model('xgboost',optimize='Accuracy')

###  Evaluate the Model
pc.evaluate_model(tuned_xgboost)

# Plot Performance of Model (AUC Default)
pc.plot_model(tuned_xgboost)

# Plot Prediction Error of Model
pc.plot_model(tuned_xgboost,plot='error')

# Plot Confusion Matrix
pc.plot_model(tuned_xgboost,plot='confusion_matrix')

# Feature Importance
pc.plot_model(tuned_xgboost,plot='feature')

# Validation Curve
pc.plot_model(tuned_xgboost,plot='vc')

# optimize threshold for trained model
pc.optimize_threshold(tuned_xgboost, true_negative = 1500, false_negative = -5000)

# Save Models
pc.save_model(tuned_xgboost,'xgb_saved_model_04072020')

# Loading the saved model
loaded_model = pc.load_model('xgb_saved_model_04072020')

# Interpret Model
pc.interpret_model(tuned_xgboost)

# Finalize Model For Prediction
final_xgb_model = pc.finalize_model(tuned_xgboost)

"""### Making A Simple Prediction with PyCaret
+ Create A Dataframe
+ Dictionary (columns_name:values)
"""

# Method 1
df.iloc[1]

df.iloc[[1]]

# We remove the target from the data
unseen_data = df.iloc[[1],:-1]

unseen_data

type(unseen_data)

# Predict with Model
prediction = pc.predict_model(final_xgb_model,data=unseen_data)

prediction

# Method 2 (Dict=>Df)
df.columns.tolist()

col_names = ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes', 'ejection_fraction', 'high_blood_pressure', 'platelets', 'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']

list(df.iloc[1])

sample_values = [55.0, 0.0, 7861.0, 0.0, 38.0, 0.0, 263358.03, 1.1, 136.0, 1.0, 0.0, 6.0]

d = dict(zip(col_names,sample_values))

d

unseen_data2 = pd.DataFrame([d])

unseen_data2

# Predict with Model
prediction2 = pc.predict_model(final_xgb_model,data=unseen_data2)

prediction2

